{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# personal coded classes\n",
    "from unet import UNet\n",
    "from apply_augmentation import DataAugmentation\n",
    "from dynamic_augmentation_pipeline import RoadSegmentationDataset\n",
    "from test_model import test_model\n",
    "from train_workflow import train_workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "testing_dir = \"test_set_images/\"\n",
    "training_dir = \"training/\"\n",
    "image_dir = os.path.join(training_dir, \"images/\")\n",
    "gt_dir = os.path.join(training_dir, \"groundtruth/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_masks(image_dir, gt_dir, valid_extensions=(\".png\", \".jpg\", \".jpeg\")):\n",
    "    \"\"\"\n",
    "    Load images and their corresponding ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Directory containing the input images.\n",
    "        gt_dir (str): Directory containing the ground truth masks.\n",
    "        valid_extensions (tuple): Tuple of valid image file extensions.\n",
    "\n",
    "    Returns:\n",
    "        list: List of images as PIL.Image objects.\n",
    "        list: List of ground truth masks as PIL.Image objects.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(valid_extensions)])\n",
    "        gt_files = sorted([f for f in os.listdir(gt_dir) if f.endswith(valid_extensions)])\n",
    "        \n",
    "        # Check if the number of images matches the number of masks\n",
    "        if len(image_files) != len(gt_files):\n",
    "            raise ValueError(\"Mismatch between number of images and ground truth masks.\")\n",
    "\n",
    "        images = [Image.open(os.path.join(image_dir, f)) for f in image_files]\n",
    "        gt_masks = [Image.open(os.path.join(gt_dir, f)) for f in gt_files]\n",
    "        \n",
    "        print(f\"Loaded {len(images)} images and {len(gt_masks)} ground truth masks.\")\n",
    "        return images, gt_masks\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading images and masks: {e}\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 images and 100 ground truth masks.\n",
      "Loaded 100 images and 100 ground truth masks.\n"
     ]
    }
   ],
   "source": [
    "images, gt_masks = load_images_and_masks(image_dir, gt_dir)\n",
    "print(f\"Loaded {len(images)} images and {len(gt_masks)} ground truth masks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key model parameters\n",
    "IMG_PATCH_SIZE = 16  # Patch size, must align with image dimensions and be a factor of image width/height\n",
    "\n",
    "# Data parameters\n",
    "TRAINING_SIZE = 0.8\n",
    "VALIDATION_SIZE = 0.2\n",
    "BATCH_SIZE = 32  # Larger batch sizes are preferred if memory allows; smaller for fine-tuning\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 50  # Start with 50 and adjust based on observed convergence behavior\n",
    "LEARNING_RATE = 1e-4  # starting point for Adam optimizer\n",
    "WEIGHT_DECAY = 1e-4  # L2 regularization to prevent overfitting\n",
    "PATIENCE = 10  # Stop training if no improvement for 10 epochs\n",
    "\n",
    "# Randomization\n",
    "SEED = 66478  # Fixed seed for reproducibility (set to None for random seed)\n",
    "\n",
    "# Model restoration\n",
    "RESTORE_MODEL = False  # Set to True if you want to resume training from a previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from a given image\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j : j + w, i : i + h]\n",
    "            else:\n",
    "                im_patch = im[j : j + w, i : i + h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images + 1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print(\"Loading \" + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print(\"File \" + image_filename + \" does not exist\")\n",
    "\n",
    "\n",
    "    img_patches = [\n",
    "        img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(len(imgs))\n",
    "    ]\n",
    "    data = [\n",
    "        img_patches[i][j]\n",
    "        for i in range(len(img_patches))\n",
    "        for j in range(len(img_patches[i]))\n",
    "    ]\n",
    "\n",
    "    return np.asarray(data)\n",
    "\n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    \"\"\"\n",
    "    Assign a label to a patch based on the number of road pixels.\n",
    "    \"\"\"\n",
    "    foreground_threshold = 0.25  # Percentage of road pixels to classify as foreground\n",
    "    df = np.mean(v)  # Calculate the proportion of road pixels\n",
    "    if df > foreground_threshold:  # More road pixels than the threshold\n",
    "        return [0, 1]  # road\n",
    "    else:  # Less road pixels\n",
    "        return [1, 0]  # background\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 2D array of class indices [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images + 1):\n",
    "        imageid = \"satImage_%.3d\" % i\n",
    "        image_filename = filename + imageid + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            print(\"Loading \" + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print(\"File \" + image_filename + \" does not exist\")\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [\n",
    "        img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)\n",
    "    ]\n",
    "\n",
    "    # Convert each patch to a single class label (0 for background, 1 for road)\n",
    "    data = np.asarray(\n",
    "        [gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))]\n",
    "    )\n",
    "    labels = np.asarray([1 if np.mean(data[i]) > 0.25 else 0 for i in range(len(data))])  # Class indices (not one-hot)\n",
    "\n",
    "    # Return as integer class indices\n",
    "    return labels.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training/images/satImage_001.png\n",
      "Loading training/images/satImage_002.png\n",
      "Loading training/images/satImage_003.png\n",
      "Loading training/images/satImage_004.png\n",
      "Loading training/images/satImage_005.png\n",
      "Loading training/images/satImage_006.png\n",
      "Loading training/images/satImage_007.png\n",
      "Loading training/images/satImage_008.png\n",
      "Loading training/images/satImage_009.png\n",
      "Loading training/images/satImage_010.png\n",
      "Loading training/images/satImage_011.png\n",
      "Loading training/images/satImage_012.png\n",
      "Loading training/images/satImage_013.png\n",
      "Loading training/images/satImage_014.png\n",
      "Loading training/images/satImage_015.png\n",
      "Loading training/images/satImage_016.png\n",
      "Loading training/images/satImage_017.png\n",
      "Loading training/images/satImage_018.png\n",
      "Loading training/images/satImage_019.png\n",
      "Loading training/images/satImage_020.png\n",
      "Loading training/images/satImage_021.png\n",
      "Loading training/images/satImage_022.png\n",
      "Loading training/images/satImage_023.png\n",
      "Loading training/images/satImage_024.png\n",
      "Loading training/images/satImage_025.png\n",
      "Loading training/images/satImage_026.png\n",
      "Loading training/images/satImage_027.png\n",
      "Loading training/images/satImage_028.png\n",
      "Loading training/images/satImage_029.png\n",
      "Loading training/images/satImage_030.png\n",
      "Loading training/images/satImage_031.png\n",
      "Loading training/images/satImage_032.png\n",
      "Loading training/images/satImage_033.png\n",
      "Loading training/images/satImage_034.png\n",
      "Loading training/images/satImage_035.png\n",
      "Loading training/images/satImage_036.png\n",
      "Loading training/images/satImage_037.png\n",
      "Loading training/images/satImage_038.png\n",
      "Loading training/images/satImage_039.png\n",
      "Loading training/images/satImage_040.png\n",
      "Loading training/images/satImage_041.png\n",
      "Loading training/images/satImage_042.png\n",
      "Loading training/images/satImage_043.png\n",
      "Loading training/images/satImage_044.png\n",
      "Loading training/images/satImage_045.png\n",
      "Loading training/images/satImage_046.png\n",
      "Loading training/images/satImage_047.png\n",
      "Loading training/images/satImage_048.png\n",
      "Loading training/images/satImage_049.png\n",
      "Loading training/images/satImage_050.png\n",
      "Loading training/images/satImage_051.png\n",
      "Loading training/images/satImage_052.png\n",
      "Loading training/images/satImage_053.png\n",
      "Loading training/images/satImage_054.png\n",
      "Loading training/images/satImage_055.png\n",
      "Loading training/images/satImage_056.png\n",
      "Loading training/images/satImage_057.png\n",
      "Loading training/images/satImage_058.png\n",
      "Loading training/images/satImage_059.png\n",
      "Loading training/images/satImage_060.png\n",
      "Loading training/images/satImage_061.png\n",
      "Loading training/images/satImage_062.png\n",
      "Loading training/images/satImage_063.png\n",
      "Loading training/images/satImage_064.png\n",
      "Loading training/images/satImage_065.png\n",
      "Loading training/images/satImage_066.png\n",
      "Loading training/images/satImage_067.png\n",
      "Loading training/images/satImage_068.png\n",
      "Loading training/images/satImage_069.png\n",
      "Loading training/images/satImage_070.png\n",
      "Loading training/images/satImage_071.png\n",
      "Loading training/images/satImage_072.png\n",
      "Loading training/images/satImage_073.png\n",
      "Loading training/images/satImage_074.png\n",
      "Loading training/images/satImage_075.png\n",
      "Loading training/images/satImage_076.png\n",
      "Loading training/images/satImage_077.png\n",
      "Loading training/images/satImage_078.png\n",
      "Loading training/images/satImage_079.png\n",
      "Loading training/images/satImage_080.png\n",
      "Loading training/images/satImage_081.png\n",
      "Loading training/images/satImage_082.png\n",
      "Loading training/images/satImage_083.png\n",
      "Loading training/images/satImage_084.png\n",
      "Loading training/images/satImage_085.png\n",
      "Loading training/images/satImage_086.png\n",
      "Loading training/images/satImage_087.png\n",
      "Loading training/images/satImage_088.png\n",
      "Loading training/images/satImage_089.png\n",
      "Loading training/images/satImage_090.png\n",
      "Loading training/images/satImage_091.png\n",
      "Loading training/images/satImage_092.png\n",
      "Loading training/images/satImage_093.png\n",
      "Loading training/images/satImage_094.png\n",
      "Loading training/images/satImage_095.png\n",
      "Loading training/images/satImage_096.png\n",
      "Loading training/images/satImage_097.png\n",
      "Loading training/images/satImage_098.png\n",
      "Loading training/images/satImage_099.png\n",
      "Loading training/images/satImage_100.png\n",
      "Loading training/groundtruth/satImage_001.png\n",
      "Loading training/groundtruth/satImage_002.png\n",
      "Loading training/groundtruth/satImage_003.png\n",
      "Loading training/groundtruth/satImage_004.png\n",
      "Loading training/groundtruth/satImage_005.png\n",
      "Loading training/groundtruth/satImage_006.png\n",
      "Loading training/groundtruth/satImage_007.png\n",
      "Loading training/groundtruth/satImage_008.png\n",
      "Loading training/groundtruth/satImage_009.png\n",
      "Loading training/groundtruth/satImage_010.png\n",
      "Loading training/groundtruth/satImage_011.png\n",
      "Loading training/groundtruth/satImage_012.png\n",
      "Loading training/groundtruth/satImage_013.png\n",
      "Loading training/groundtruth/satImage_014.png\n",
      "Loading training/groundtruth/satImage_015.png\n",
      "Loading training/groundtruth/satImage_016.png\n",
      "Loading training/groundtruth/satImage_017.png\n",
      "Loading training/groundtruth/satImage_018.png\n",
      "Loading training/groundtruth/satImage_019.png\n",
      "Loading training/groundtruth/satImage_020.png\n",
      "Loading training/groundtruth/satImage_021.png\n",
      "Loading training/groundtruth/satImage_022.png\n",
      "Loading training/groundtruth/satImage_023.png\n",
      "Loading training/groundtruth/satImage_024.png\n",
      "Loading training/groundtruth/satImage_025.png\n",
      "Loading training/groundtruth/satImage_026.png\n",
      "Loading training/groundtruth/satImage_027.png\n",
      "Loading training/groundtruth/satImage_028.png\n",
      "Loading training/groundtruth/satImage_029.png\n",
      "Loading training/groundtruth/satImage_030.png\n",
      "Loading training/groundtruth/satImage_031.png\n",
      "Loading training/groundtruth/satImage_032.png\n",
      "Loading training/groundtruth/satImage_033.png\n",
      "Loading training/groundtruth/satImage_034.png\n",
      "Loading training/groundtruth/satImage_035.png\n",
      "Loading training/groundtruth/satImage_036.png\n",
      "Loading training/groundtruth/satImage_037.png\n",
      "Loading training/groundtruth/satImage_038.png\n",
      "Loading training/groundtruth/satImage_039.png\n",
      "Loading training/groundtruth/satImage_040.png\n",
      "Loading training/groundtruth/satImage_041.png\n",
      "Loading training/groundtruth/satImage_042.png\n",
      "Loading training/groundtruth/satImage_043.png\n",
      "Loading training/groundtruth/satImage_044.png\n",
      "Loading training/groundtruth/satImage_045.png\n",
      "Loading training/groundtruth/satImage_046.png\n",
      "Loading training/groundtruth/satImage_047.png\n",
      "Loading training/groundtruth/satImage_048.png\n",
      "Loading training/groundtruth/satImage_049.png\n",
      "Loading training/groundtruth/satImage_050.png\n",
      "Loading training/groundtruth/satImage_051.png\n",
      "Loading training/groundtruth/satImage_052.png\n",
      "Loading training/groundtruth/satImage_053.png\n",
      "Loading training/groundtruth/satImage_054.png\n",
      "Loading training/groundtruth/satImage_055.png\n",
      "Loading training/groundtruth/satImage_056.png\n",
      "Loading training/groundtruth/satImage_057.png\n",
      "Loading training/groundtruth/satImage_058.png\n",
      "Loading training/groundtruth/satImage_059.png\n",
      "Loading training/groundtruth/satImage_060.png\n",
      "Loading training/groundtruth/satImage_061.png\n",
      "Loading training/groundtruth/satImage_062.png\n",
      "Loading training/groundtruth/satImage_063.png\n",
      "Loading training/groundtruth/satImage_064.png\n",
      "Loading training/groundtruth/satImage_065.png\n",
      "Loading training/groundtruth/satImage_066.png\n",
      "Loading training/groundtruth/satImage_067.png\n",
      "Loading training/groundtruth/satImage_068.png\n",
      "Loading training/groundtruth/satImage_069.png\n",
      "Loading training/groundtruth/satImage_070.png\n",
      "Loading training/groundtruth/satImage_071.png\n",
      "Loading training/groundtruth/satImage_072.png\n",
      "Loading training/groundtruth/satImage_073.png\n",
      "Loading training/groundtruth/satImage_074.png\n",
      "Loading training/groundtruth/satImage_075.png\n",
      "Loading training/groundtruth/satImage_076.png\n",
      "Loading training/groundtruth/satImage_077.png\n",
      "Loading training/groundtruth/satImage_078.png\n",
      "Loading training/groundtruth/satImage_079.png\n",
      "Loading training/groundtruth/satImage_080.png\n",
      "Loading training/groundtruth/satImage_081.png\n",
      "Loading training/groundtruth/satImage_082.png\n",
      "Loading training/groundtruth/satImage_083.png\n",
      "Loading training/groundtruth/satImage_084.png\n",
      "Loading training/groundtruth/satImage_085.png\n",
      "Loading training/groundtruth/satImage_086.png\n",
      "Loading training/groundtruth/satImage_087.png\n",
      "Loading training/groundtruth/satImage_088.png\n",
      "Loading training/groundtruth/satImage_089.png\n",
      "Loading training/groundtruth/satImage_090.png\n",
      "Loading training/groundtruth/satImage_091.png\n",
      "Loading training/groundtruth/satImage_092.png\n",
      "Loading training/groundtruth/satImage_093.png\n",
      "Loading training/groundtruth/satImage_094.png\n",
      "Loading training/groundtruth/satImage_095.png\n",
      "Loading training/groundtruth/satImage_096.png\n",
      "Loading training/groundtruth/satImage_097.png\n",
      "Loading training/groundtruth/satImage_098.png\n",
      "Loading training/groundtruth/satImage_099.png\n",
      "Loading training/groundtruth/satImage_100.png\n",
      "Extracted 62500 image patches of size 16x16\n",
      "Extracted 62500 label patches of size 16x16\n"
     ]
    }
   ],
   "source": [
    "# Extract patches and labels\n",
    "image_patches = extract_data(image_dir, len(images))\n",
    "label_patches = extract_labels(gt_dir, len(gt_masks))\n",
    "\n",
    "print(f\"Extracted {image_patches.shape[0]} image patches of size {IMG_PATCH_SIZE}x{IMG_PATCH_SIZE}\")\n",
    "print(f\"Extracted {label_patches.shape[0]} label patches of size {IMG_PATCH_SIZE}x{IMG_PATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Patches Tensor Shape: torch.Size([62500, 3, 16, 16])\n",
      "Label Patches Tensor Shape: torch.Size([62500])\n"
     ]
    }
   ],
   "source": [
    "# Convert image and label patches to PyTorch tensors\n",
    "image_patches = torch.from_numpy(image_patches).float().permute(0, 3, 1, 2)  # Shape: [batch_size, num_channels, patch_size, patch_size]\n",
    "label_patches = torch.from_numpy(label_patches).long() # Shape: [batch_size]\n",
    "\n",
    "# Print the shapes of tensors\n",
    "print(f\"Image Patches Tensor Shape: {image_patches.shape}\")\n",
    "print(f\"Label Patches Tensor Shape: {label_patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images: 50000, Training Labels: 50000\n",
      "Validation Images: 12500, Validation Labels: 12500\n"
     ]
    }
   ],
   "source": [
    "# Split into training and validation datasets\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    image_patches, label_patches, test_size=VALIDATION_SIZE, random_state=SEED\n",
    ")\n",
    "\n",
    "# Print the sizes of training and validation sets\n",
    "print(f\"Training Images: {train_images.shape[0]}, Training Labels: {train_labels.shape[0]}\")\n",
    "print(f\"Validation Images: {val_images.shape[0]}, Validation Labels: {val_labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the computation device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net model instantiated and moved to: cuda\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the U-Net model and move it to the device\n",
    "model = UNet().to(device)\n",
    "print(\"U-Net model instantiated and moved to:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([0.668, 1.987]).to(device) # Class weights for imbalance\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Initialize ReduceLROnPlateau scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",              # Maximize the F1 Score\n",
    "    factor=0.5,              # Reduce LR by a factor of 0.5\n",
    "    patience=3,              # Wait for 3 epochs without improvement\n",
    "    threshold=1e-4,          # Minimal change to qualify as improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data augmentation\n",
    "data_augmentation = DataAugmentation(img_width = 400, img_height = 400, patch_size = IMG_PATCH_SIZE)\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = RoadSegmentationDataset(\n",
    "    images=train_images, \n",
    "    labels=train_labels, \n",
    "    augmentations=data_augmentation, \n",
    ")\n",
    "val_dataset = RoadSegmentationDataset(\n",
    "    images=val_images, \n",
    "    labels=val_labels, \n",
    "    augmentations=None, \n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the training workflow\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m train_workflow(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      4\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m      5\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m      6\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m      7\u001b[0m     criterion\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m      8\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[0;32m      9\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     10\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS,\n\u001b[0;32m     11\u001b[0m     patience\u001b[38;5;241m=\u001b[39mPATIENCE,\n\u001b[0;32m     12\u001b[0m     save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroad_segmentation_best_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\OneDrive\\Bureau\\ml-project-2-quantum_minds\\train_workflow.py:46\u001b[0m, in \u001b[0;36mtrain_workflow\u001b[1;34m(model, train_loader, val_loader, optimizer, criterion, scheduler, device, num_epochs, patience, save_path)\u001b[0m\n\u001b[0;32m     43\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[0;32m     49\u001b[0m val_loss, val_f1, val_acc \u001b[38;5;241m=\u001b[39m validate_model(model, val_loader, device, criterion)\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\OneDrive\\Bureau\\ml-project-2-quantum_minds\\train_model.py:16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     15\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     17\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\OneDrive\\Bureau\\ml-project-2-quantum_minds\\dynamic_augmentation_pipeline.py:40\u001b[0m, in \u001b[0;36mRoadSegmentationDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Apply training or validation transformations\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmentations:\n\u001b[1;32m---> 40\u001b[0m     image, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmentations\u001b[38;5;241m.\u001b[39mapply_train_transforms(image, label)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\OneDrive\\Bureau\\ml-project-2-quantum_minds\\apply_augmentation.py:76\u001b[0m, in \u001b[0;36mDataAugmentation.apply_train_transforms\u001b[1;34m(self, image, label)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03mApply the same random training transformations to both image and label.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Transformed label (flat format).\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Convert label to spatial format\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m label_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_to_img(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_height, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, label)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Apply transformations\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Horizontal flip\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Abdou2\\OneDrive\\Bureau\\ml-project-2-quantum_minds\\apply_augmentation.py:38\u001b[0m, in \u001b[0;36mDataAugmentation.label_to_img\u001b[1;34m(self, img_width, img_height, patch_size, labels)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, img_height, patch_size):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, img_width, patch_size):\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;66;03m# Assign patch label\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m         l \u001b[38;5;241m=\u001b[39m labels[idx]\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Binary label (0 or 1)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         array_labels[i:i \u001b[38;5;241m+\u001b[39m patch_size, j:j \u001b[38;5;241m+\u001b[39m patch_size] \u001b[38;5;241m=\u001b[39m l\n\u001b[0;32m     40\u001b[0m         idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "# Run the training workflow\n",
    "history = train_workflow(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    save_path=\"road_segmentation_best_model.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_images(testing_dir, valid_extensions=(\".png\", \".jpg\", \".jpeg\")):\n",
    "    \"\"\"\n",
    "    Load test images from the directory.\n",
    "\n",
    "    Args:\n",
    "        testing_dir (str): Path to the directory containing test image folders.\n",
    "        valid_extensions (tuple): Valid image file extensions.\n",
    "\n",
    "    Returns:\n",
    "        list: List of test images as PIL.Image objects.\n",
    "        list: Corresponding file names for identification.\n",
    "    \"\"\"\n",
    "    test_images = []\n",
    "    test_filenames = []\n",
    "    \n",
    "    for folder in sorted(os.listdir(testing_dir)):\n",
    "        folder_path = os.path.join(testing_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            image_filename = f\"{folder}.png\"\n",
    "            image_path = os.path.join(folder_path, image_filename)\n",
    "            if os.path.isfile(image_path) and image_filename.endswith(valid_extensions):\n",
    "                try:\n",
    "                    img = Image.open(image_path).convert(\"RGB\")  # Ensure images are RGB\n",
    "                    test_images.append(img)\n",
    "                    test_filenames.append(folder)  # Save folder name for identification\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {image_path}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {len(test_images)} test images.\")\n",
    "    return test_images, test_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = np.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0, imgheight, h):\n",
    "        for j in range(0, imgwidth, w):\n",
    "            # Assign class label based on the first channel\n",
    "            if labels[idx][0] > 0.5:  # background\n",
    "                l = 0\n",
    "            else:  # road\n",
    "                l = 1\n",
    "            # Fill the region in the image with the assigned label\n",
    "            array_labels[j:j + w, i:i + h] = l\n",
    "            idx += 1\n",
    "    return array_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_float_to_uint8(img):\n",
    "    rimg = img - np.min(img)\n",
    "    rimg = (rimg / np.max(rimg) * PIXEL_DEPTH).round().astype(np.uint8)\n",
    "    return rimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_images(img, gt_img):\n",
    "    n_channels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if n_channels == 3:\n",
    "        cimg = np.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)\n",
    "        gt_img_3c[:, :, 0] = gt_img8\n",
    "        gt_img_3c[:, :, 1] = gt_img8\n",
    "        gt_img_3c[:, :, 2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_img_overlay(img, predicted_img):\n",
    "    \"\"\"\n",
    "    Create an overlay of predictions on the input image.\n",
    "    \"\"\"\n",
    "    w, h = img.shape[:2]\n",
    "    color_mask = np.zeros((w, h, 3), dtype=np.uint8)\n",
    "    color_mask[:, :, 0] = predicted_img.astype(np.uint8) * 255  # Red channel for predictions\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, \"RGB\").convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, \"RGB\").convert(\"RGBA\")\n",
    "    return Image.blend(background, overlay, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
